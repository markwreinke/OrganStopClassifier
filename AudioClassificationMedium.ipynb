{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioClassificationMedium.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN2UKsRwsPsy9oapyECWSym",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b8ea44331c0f4f0aaec62f8623e45bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_63622b91beb343dabffafb656da44354",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_feab427d4c0a41fa90a7d902e022d3a2",
              "IPY_MODEL_fb61b82b529b425da50b2bfd816d7822"
            ]
          }
        },
        "63622b91beb343dabffafb656da44354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "feab427d4c0a41fa90a7d902e022d3a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a9561062c38d4520a1667862d282ae6b",
            "_dom_classes": [],
            "description": " 27%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1600,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 432,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1efb6924ba3a474abf9856f20b828e2c"
          }
        },
        "fb61b82b529b425da50b2bfd816d7822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0c553b8cdb59471c8ff52d4540e18c22",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 431/1600 [01:20&lt;03:47,  5.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11359444ff104b08a8848fa73eb0be59"
          }
        },
        "a9561062c38d4520a1667862d282ae6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1efb6924ba3a474abf9856f20b828e2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c553b8cdb59471c8ff52d4540e18c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11359444ff104b08a8848fa73eb0be59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markwreinke/OrganStopClassifier/blob/main/AudioClassificationMedium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJX6CnV53VA"
      },
      "source": [
        "This is a tutorial taken from https://medium.com/@hasithsura/audio-classification-d37a82d6715 \n",
        "\n",
        "Be sure to look at the github, as the article doesn't have the whole code: https://github.com/hasithsura/Environmental-Sound-Classification/blob/master/ESC50-Pytorch.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpfTjsnckRp2"
      },
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I79dQ3pqRg3u",
        "outputId": "c5dfa739-cce4-4c26-d827-018aca62a439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mounting google drive into colab, and ensuring we are in the data folder\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "dir = \"/content/drive/My Drive/ESC-50-master (1)\"\n",
        "print(os.getcwd())\n",
        "\n",
        "if(os.getcwd() != dir):\n",
        "  os.chdir(dir)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/ESC-50-master (1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR62F6pOkQz8"
      },
      "source": [
        "# Here we are reading a comma separated values (CSV) file using pandas into the \n",
        "# variable df. df is type a 2D data structure with labeled axes, \n",
        "# as a DataFrame or TextParser\n",
        "df = pd.read_csv('meta/esc50.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrhJn8L9k-Ca",
        "outputId": "33a6bf4e-5152-4291-a977-1dddf2ca67dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# Return the first n rows of the DataFrame. n defaults to 5 with no args\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>fold</th>\n",
              "      <th>target</th>\n",
              "      <th>category</th>\n",
              "      <th>esc10</th>\n",
              "      <th>src_file</th>\n",
              "      <th>take</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1-100032-A-0.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>dog</td>\n",
              "      <td>True</td>\n",
              "      <td>100032</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1-100038-A-14.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>chirping_birds</td>\n",
              "      <td>False</td>\n",
              "      <td>100038</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1-100210-A-36.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>vacuum_cleaner</td>\n",
              "      <td>False</td>\n",
              "      <td>100210</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1-100210-B-36.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>vacuum_cleaner</td>\n",
              "      <td>False</td>\n",
              "      <td>100210</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1-101296-A-19.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>thunderstorm</td>\n",
              "      <td>False</td>\n",
              "      <td>101296</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            filename  fold  target        category  esc10  src_file take\n",
              "0   1-100032-A-0.wav     1       0             dog   True    100032    A\n",
              "1  1-100038-A-14.wav     1      14  chirping_birds  False    100038    A\n",
              "2  1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A\n",
              "3  1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B\n",
              "4  1-101296-A-19.wav     1      19    thunderstorm  False    101296    A"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M29rKg8lfOBp"
      },
      "source": [
        "# The data was sorted into 5 \"Cross-validation\" folds, in which here, the first\n",
        "# four are used as training data, and the fifth is a validation set\n",
        "# Here's the paper: http://karol.piczak.com/papers/Piczak2015-ESC-Dataset.pdf\n",
        "train = df[df['fold']!=5]\n",
        "valid = df[df['fold']==5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PtZtZ5pewjd",
        "outputId": "85d386a1-c0e0-461c-d8c4-8be1de84cf33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load a filewith librosa, and display its sampling rate and length\n",
        "wav, sr = librosa.load('audio/1-100032-A-0.wav', sr=None)\n",
        "print(f'Sampling rate of the audio is {sr} and length of the audio is {len(wav)/sr} seconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sampling rate of the audio is 44100 and length of the audio is 5.0 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUui5w8H4djV"
      },
      "source": [
        "# Here we are going to convert a Spectrogram into an image\n",
        "def spec_to_image(spec, eps=1e-6):\n",
        "  mean = spec.mean()\n",
        "  std = spec.std()\n",
        "  spec_norm = (spec - mean) / (std + eps)\n",
        "  spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
        "  spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
        "  spec_scaled = spec_scaled.astype(np.uint8)\n",
        "  return spec_scaled\n",
        "\n",
        "  # This spectrogram is normalized using z score normalization and \n",
        "  # scaled using min-max scaling so its values lie between 0 and 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfXLoXQucv6"
      },
      "source": [
        "# This function give a mel spectrogram of the given file\n",
        "#  sr=None -> Librosa should use the native sampling rate of 44.1KHz to load the audio data instead of the default of 22.05KHz\n",
        "#  Next the first 5 seconds of given audio is extracted\n",
        "#  2048 samples are chosen for each window (about 46ms)\n",
        "#  A hop_length of 512 samples is chose, meaning the window is moved by skipping 512 samples to get the next time frame\n",
        "#  The number of mel filters is 128, makes the height of the spectrogram image 128\n",
        "#  fmin and fmax are the lowest and highest frequencies\n",
        "def get_melspectrogram_db(file_path, sr=None, n_fft=2048, hop_length=512, n_mels=128, fmin=20, fmax=8300, top_db=80):\n",
        "  wav, sr = librosa.load(file_path, sr=sr)\n",
        "  if wav.shape[0]<5*sr:\n",
        "     wav=np.pad(wav,int(np.ceil((5*sr-wav.shape[0])/2)),mode='reflect')\n",
        "  else:\n",
        "    wav=wav[:5*sr]\n",
        "\n",
        "  spec=librosa.feature.melspectrogram(wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
        "\n",
        "  #Librosa squares the magnitude of the spectrogram when constructing Mel Spectrogram, so we use power_to_db to convert power magnitude to decipels. top_db is used to threshold the output.\n",
        "  spec_db=librosa.power_to_db(spec, top_db=top_db)\n",
        "  return spec_db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdBLjGXS5_jo",
        "outputId": "977b5b84-08fb-4681-cdd2-226272136f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "b8ea44331c0f4f0aaec62f8623e45bb4",
            "63622b91beb343dabffafb656da44354",
            "feab427d4c0a41fa90a7d902e022d3a2",
            "fb61b82b529b425da50b2bfd816d7822",
            "a9561062c38d4520a1667862d282ae6b",
            "1efb6924ba3a474abf9856f20b828e2c",
            "0c553b8cdb59471c8ff52d4540e18c22",
            "11359444ff104b08a8848fa73eb0be59"
          ]
        }
      },
      "source": [
        "# This is us loading data into pytorch by building dataloaders to\n",
        "# preprocess and load data.\n",
        "\n",
        "class ESC50Data(Dataset):\n",
        "  def __init__(self, base, df, in_col, out_col):\n",
        "    self.df = df\n",
        "    self.data = []\n",
        "    self.labels = []\n",
        "    self.c2i = {}\n",
        "    self.i2c = {}\n",
        "    self.categories = sorted(df[out_col].unique())\n",
        "    for i, category in enumerate(self.categories):\n",
        "      self.c2i[category] = i\n",
        "      self.i2c[i] = category\n",
        "    for ind in tqdm(range(len(df))):\n",
        "      row = df.iloc[ind]\n",
        "      file_path = os.path.join(base, row[in_col])\n",
        "      self.data.append(spec_to_image(get_melspectrogram_db(file_path))[np.newaxis,...])\n",
        "      self.labels.append(self.c2i[row['category']])\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "      return self.data[idx], self.labels[idx]\n",
        "\n",
        "train_data = ESC50Data('audio', train, 'filename', 'category')\n",
        "valed_data = ESC50Data('audio', valid, 'filename', 'category')\n",
        "train_loader = DataLoader(train_data, batch_size = 16, shuffle = True)\n",
        "valid_loader = DataLoader(valid_data, batch_size = 16, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8ea44331c0f4f0aaec62f8623e45bb4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1600.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmOWZWaXdezM"
      },
      "source": [
        "# Build a model using a pre-trained model, resnet34\n",
        "from torchvision.models import resnet34\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "resnet_model = resnet34(pretrained = True)\n",
        "resnet_model.fc = nn.Linear(512, 50)\n",
        "resnet_model.conv1 = nn.Conv2d(1, 64, kernel_size = (7,7), stride=(2,2), padding = (3, 3), bias = False)\n",
        "resnet_model = resnet_model.to(device)\n",
        "\n",
        "# The first conv1 layer of resnet34 accepts 3 channels so it is changed to accept 1 channel.\n",
        "# The final fc layer generates output for 1000 categoris so it is changed to 50 categories\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APCcWrwBrU9A"
      },
      "source": [
        "# Train the model\n",
        "learning_rate = 2e-4\n",
        "optimizer = optim.Adam(resnet_model.parameters(), lr = learning_rate)\n",
        "epochs = 50\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "resnet_train_losses=[]\n",
        "resnet_valid_losses=[]\n",
        "def lr_decay(optimizer, epoch):\n",
        "  if epoch%10==0:\n",
        "    new_lr = learning_rate / (10**(epoch//10))\n",
        "    optimizer = setlr(optimizer, new_lr)\n",
        "    print(f'Changed learning rate to {new_lr}')\n",
        "  return optimizer\n",
        "\n",
        "  def train(model, loss_fn, train_loser, valid_loader, epochs, optimizer, train_losses, valid_losses, change_lr = None):\n",
        "    for epoch in tqdm(range(1,epochs+1)):\n",
        "      model.train()\n",
        "      batch_losses=[]\n",
        "      if change_lr:\n",
        "        optimizer = change_lr(optimizer, epoch)\n",
        "      for i, data in enumerate(train_loader):\n",
        "        x,y = data\n",
        "        optimizer.zero_grad()\n",
        "        x = x.to(device, dtype = torch.float32)\n",
        "        y = y.to(device, dtype = torch.long)\n",
        "        y_hat = model(x)\n",
        "        loss = loss_fn(y_hat, y)\n",
        "        loss.backward()\n",
        "        batch_losses.append(loss.item())\n",
        "        optimizer.step()\n",
        "\n",
        "      train_losses.append(batch_losses)\n",
        "      print(f'Epoch = {epoch} Train-Loss : {np.mean(train_losses[-1])}')\n",
        "      model.eval()\n",
        "      batch_losses=[]\n",
        "      trace_y = []\n",
        "      trace_yhat = []\n",
        "      for i, data in enumerate(valid_loader):\n",
        "        x,y = data\n",
        "        x = x.to(device, dtype=torch.float32)\n",
        "        y = y.to(device, dtype = torch.long)\n",
        "        y_hat = model(x)\n",
        "        loss = loss_fn(y_hat, y)\n",
        "        trace_y.append(y.cpu().detach().numpy())\n",
        "        batch_losses.append(loss.item())\n",
        "\n",
        "      valid_losses.append(batch_losses)\n",
        "      trace_y = np.concatenate(trace_y)\n",
        "      trace_yhat = np.concatenate(trace_yhat)\n",
        "      accuracy = np.mean(trace_yhat.argmax(axis = 1) == trace_y)\n",
        "      print(f'Epoch - {epoch} Valid-Loss : {np.mean(valid_losses[-1])} Valid-Accuracy : {accuracy}')\n",
        "\n",
        "train(resnet_model, loss_fn, train_loader, valid_loader, epochs, optimizer, resnet_train_losses, resnet_valid_losses, lr_decay)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}